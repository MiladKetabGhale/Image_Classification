{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f85feb8-b54d-4679-ace9-9c3e71a90cfc",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "## 1. Introduction\n",
    "   - 1.1. Overview of the EMNIST Dataset\n",
    "     - Description and Importance\n",
    "     - Structure and Dataset Variants\n",
    "   - 1.2. Initial Exploratory Data Analysis (EDA)\n",
    "     - 1.2.1. Essential Dataset Insights\n",
    "       - Balanced Digit Distribution  \n",
    "       - Visualization of Random Samples\n",
    "       - Images Analysis: Pixel Density, Centre of Mass, Non-zero Proportions \n",
    "     - 1.2.2. Variance and Class-Level Pixel Statistics  \n",
    "       - Feature Variance Across Digits  \n",
    "       - Mean and Variance Images per Digit  \n",
    "       - Visualizing Differences in Average Images\n",
    "\n",
    "# Part I: Supervised Classification\n",
    "\n",
    "## 2. Training and Evaluation of Baseline Models\n",
    "   - 2.1. Helper Functionality for Baselining\n",
    "   - 2.2. Baseline Models\n",
    "      - 2.2.1. Logistic Regression\n",
    "      - 2.2.2. Support Vector Machine (SVM)\n",
    "      - 2.2.3. Decision Tree\n",
    "      - 2.2.4. Ensemble Methods\n",
    "         - 2.2.4.1. Random Forest\n",
    "         - 2.2.4.2. Extreme Trees\n",
    "         - 2.2.4.3. XGBoost\n",
    "         - 2.2.4.4. AdaBoost\n",
    "   - 2.3. Performance Evaluation: A Risk Measurement Prespective\n",
    "      - 2.3.1. Helper Functionality for Modular Evaluation and Summarization\n",
    "      - 2.3.2. An Initial Overview of Baseline Models Performance\n",
    "      - 2.3.3. Analysis of Precision-Recall Curve\n",
    "      - 2.3.4. Looking into Precision-Recall-Threshold\n",
    "      - 2.3.5. Understanding ROC Curve\n",
    "      - 2.3.6. F1-Score vs Threshold\n",
    "      - 2.3.2. Comparative Analysis (Merged with 2.3.1)\n",
    "   - 2.4. Reliability Analysis: Brier Scores\n",
    "   - 2.5. Error Analysis\n",
    "      - 2.5.1. Confusion Matrices Analysis: One-vs-All\n",
    "      - 2.5.2. Confusion Heatmaps Analysis: One-vs-One\n",
    "         - 2.5.2.1. True Negatives: The Models' Strengths\n",
    "         - 2.5.2.2. False Negatives: The Toughest Challenges\n",
    "         - 2.5.2.3. False Positives: The Confusing Lookalikes\n",
    "      - 2.5.3. Visualization of Misclassified Images\n",
    "         - 2.5.3.1. A Look into Misclassified By the Linear Models\n",
    "         - 2.5.3.2. Seeing the Misclassified By the Tree-based Models\n",
    "         - 2.5.3.3. Images Misclassified By the Boosting Models\n",
    "         - 2.5.3.4. Poor Images Misunderstood By Every Model\n",
    "   - 2.6. Strategic Insights and Forward Planning\n",
    "\n",
    "## 3. From Baseline to Brilliance: Classifiers Optimization\n",
    "   - 3.1. Role of Hyperparameter Tuning and Cross Validation\n",
    "      - 3.1.1. Machine Learning As Function Approximation\n",
    "      - 3.1.2. Learning As Optimization Over Function Space\n",
    "      - 3.1.3. Hyperparameter Tuning As Searching Strategy\n",
    "      - 3.1.4. Quantifying Generalizability Using Cross Validation\n",
    "         - 3.1.4.1. The Bias-Variance Trade-off\n",
    "         - 3.1.4.2. Tyoes of Cross Validation \n",
    "   - 3.2. Hyperparameter Tuning Strategies: the Exploration vs Exploitation Paradigm\n",
    "      - 3.2.1. Helper Funtionality for Modularization\n",
    "      - 3.2.2. HalvingGridSearchCV\n",
    "      - 3.2.3. Specs, Training, Evaluation, and Quantification of Tuned Models Against the Baselines\n",
    "      - 3.2.4. Remarks and Insights for Further Tuning\n",
    "   - 3.3. Performance and Timing Analysis For Tuned Models\n",
    "      - 3.3.1. Overview of the Results\n",
    "      - 3.3.2. Precision-Recall: A Pareto Efficiency Perspective\n",
    "          - 3.3.2.1. Key Observations\n",
    "          - 3.3.2.2. Cost-benefit Analysis\n",
    "          - 3.3.2.3. From Theory to Business\n",
    "      - 3.3.3. Precision-Recall-Threshold Curve\n",
    "      - 3.3.4. ROC Curve: Trade-off Between Sensitivity and Selectivity\n",
    "      - 3.3.5. F1-score vs Threshold\n",
    "   - 3.4. Error Analysis\n",
    "      - 3.4.1. Confusion Matrix Analysis: The Binary Case of 8-vs-all\n",
    "      - 3.4.2. Class-wise Confusion Matrix Analysis\n",
    "          - 3.4.2.1. False Positive Analysis\n",
    "          - 3.4.2.2. True Negative Analysis\n",
    "      - 3.4.3. Common Errors and Insights: Identifying, Visualizing, and Analyzing\n",
    "      - 3.4.4. Reliability Analysis\n",
    "         - 3.4.4.1. Brier Scores\n",
    "         - 3.4.4.2. Calibration Curves\n",
    "\n",
    "## 4. The Quest for the Best Classifier\n",
    "   - 4.1. Voting Ensembles: An Overview\n",
    "   - 4.2. Training Hard and Soft Voting Models\n",
    "      - 4.2.1. Evaluation of the Voting Models\n",
    "   - 4.3. Training A Stack Ensemble\n",
    "      - 4.3.2. Meta-model Selection and Training\n",
    "      - 4.3.1. Evaluation of the Stack Model \n",
    "   - 4.4. Quantitative Comparison of the Voting Ensembles and the Stack Model\n",
    "   - 4.5. Error Distribution Analysis\n",
    "      - 4.5.1. Missclassified Frequencies\n",
    "      - 4.5.2. Heatmap Representation of Missclassified Frequencies\n",
    "      - 4.5.3. Analysis of Proportional Missclassifications\n",
    "   - 4.6. Model Generalizability Analysis\n",
    "\n",
    "## 5. Feature Importance and Feature Selection\n",
    "   - 5.1. Introduction to Feature Importance\n",
    "      - Techniques Overview (SHAP, Random Forest Importance, Coefficients in Linear Models)\n",
    "      - 5.1.1. Strengths and Weaknesses of Different Methods\n",
    "   - 5.2. Measuring Feature Importance\n",
    "      - 5.2.1. SHAP Values\n",
    "        - 5.2.1.1 Class-wise SHAP Analysis\n",
    "        - 5.2.1.2. Consistently Top Features\n",
    "      - 5.2.2. Random Forest Importance\n",
    "        - 5.2.2.1. Static Feature Importance\n",
    "        - 5.2.2.2. Dynamic Feature Importance: RFE\n",
    "   - 5.3. Feature Selection\n",
    "      - 5.3.1. Selecting Feature Sets Based on Different Techniques\n",
    "      - 5.3.2. Visually Comparing Different Feature Selections\n",
    "      - 5.3.3. Benchmarking Different Techniques: Retraining and Evaluation\n",
    "   - 5.4. Understanding Error Through Feature Importance\n",
    "      - 5.4.1. A Future Endeavour: Trustworthiness, Robustness, And Adversarial Training\n",
    "\n",
    "## 6. Training Multiclass Classifiers\n",
    "   - 6.1. Ways of Training a Multiclass Classifier\n",
    "      - 6.1.1. Comparative Advantages and Disadvantages\n",
    "   - 6.2. Training Direct Multiclass Classifiers\n",
    "      - 6.2.1. Feature Selection Pipeline\n",
    "      - 6.2.2. Modular Evaluation Functionality\n",
    "      - 6.2.3. Naming Convention\n",
    "      - 6.2.4. XGBoost\n",
    "      - 6.2.5. AdaBoost\n",
    "      - 6.2.6. LightGBM\n",
    "      - 6.2.7. Benchmarking the Direct Models\n",
    "         - 6.2.7.1. Analyzing Precision And Recall For All Models and Digits\n",
    "         - 6.2.7.2. Discussion on AdaBoost's Multiclass Classifer Underperformance\n",
    "   - 6.3. Training and Ensembling An AdaBoost Multiclass Classifier\n",
    "      - 6.3.1. Systematic Training Multiple One-vs-All AdaBoost Models\n",
    "      - 6.3.2. A Soft Voting AdaBoost Ensemble\n",
    "      - 6.3.3. Benchmarking The Multiclass Soft Voting Ensemble\n",
    "         - 6.3.3.1. The Ensemble of One-vs-All Multiclass Classifer vs The Multiclass AdaBoost Classifier\n",
    "   - 6.4. Why The Ensemble of One-vs-All Won?\n",
    "   - 6.5. Benchamrking The Soft Voting Ensemble vs XGB and LightGBM Multiclass Classifiers\n",
    "   - 6.6. Error Analysis and Visualizations\n",
    "   - 6.7. Quantifying Model Generalizability and Uncertainty\n",
    "      - 6.7.1. Training, Validation, And Test Curves\n",
    "      - 6.7.2. Measuring Uncertainty\n",
    "         - 6.7.2.1. Predictive Entropy\n",
    "         - 6.7.2.2. Quantifiably Distinguishing Between Overconfidence And Uncertainty   \n",
    "\n",
    "## 7. Conclusion\n",
    "   - 7.1. Limitations and Future Work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
